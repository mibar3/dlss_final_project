{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0c3611a-1887-4ddd-a4b4-8d8cf71fffce",
   "metadata": {},
   "source": [
    "# New sequences creation, LSTM+CNN with multiple district outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "564a04e8-ee24-4078-8ab8-03dad38723cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Flatten, Dense, LSTM, TimeDistributed, Dropout,\n",
    "    Input, InputLayer, concatenate, GlobalMaxPooling2D, Reshape\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import cv2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "89d0e815-988a-4f3a-922c-f1e82cb34a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7800353 entries, 0 to 7800352\n",
      "Data columns (total 38 columns):\n",
      " #   Column                         Dtype  \n",
      "---  ------                         -----  \n",
      " 0   Date                           object \n",
      " 1   IUCR                           object \n",
      " 2   Primary Type                   object \n",
      " 3   Latitude                       float64\n",
      " 4   Longitude                      float64\n",
      " 5   Month                          int64  \n",
      " 6   Season                         object \n",
      " 7   Year                           int64  \n",
      " 8   District                       int64  \n",
      " 9   Beat                           int64  \n",
      " 10  geometry                       object \n",
      " 11  Community Name                 object \n",
      " 12  Community Area Number          int64  \n",
      " 13  NIBRS                          object \n",
      " 14  Description                    object \n",
      " 15  Category                       object \n",
      " 16  Crime Against Category         object \n",
      " 17  district_name                  object \n",
      " 18  temperature_2m_max_day         float64\n",
      " 19  temperature_2m_min_day         float64\n",
      " 20  apparent_temperature_mean_day  float64\n",
      " 21  daylight_duration_day          float64\n",
      " 22  sunshine_duration_day          float64\n",
      " 23  precipitation_sum_day          float64\n",
      " 24  rain_sum_day                   float64\n",
      " 25  snowfall_sum_day               float64\n",
      " 26  precipitation_hours_day        float64\n",
      " 27  apparent_temperature_hour      float64\n",
      " 28  precipitation_hour             float64\n",
      " 29  rain_hour                      float64\n",
      " 30  snowfall_hour                  float64\n",
      " 31  cloud_cover_hour               float64\n",
      " 32  wind_speed_100m_hour           float64\n",
      " 33  Hour                           int64  \n",
      " 34  DayOfWeek                      int64  \n",
      " 35  TOT_POP                        float64\n",
      " 36  UNEMP                          float64\n",
      " 37  INC_LT_25K                     float64\n",
      "dtypes: float64(20), int64(7), object(11)\n",
      "memory usage: 2.2+ GB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"/home/scc/miranda.barros-everett/chicago_with_census_2018_2024.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66eb07bb-65f9-4dec-a6ab-8936d4e9d519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#filter year(s)\n",
    "df = df.loc[df['Year'].isin([2018, 2019]), :] #18 und 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e70d6b5-4cec-4c9a-8f50-931d24757b21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique districts: 23\n",
      "Counts of each district:\n",
      "11    36109\n",
      "6     31722\n",
      "8     30737\n",
      "18    30541\n",
      "1     29954\n",
      "4     26414\n",
      "7     26395\n",
      "12    25182\n",
      "25    25139\n",
      "10    23968\n",
      "3     23386\n",
      "19    23336\n",
      "2     21911\n",
      "5     21554\n",
      "9     20932\n",
      "15    19206\n",
      "14    17970\n",
      "16    16359\n",
      "22    15774\n",
      "24    15084\n",
      "17    13866\n",
      "20     9017\n",
      "31       18\n",
      "Name: District, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count unique values in 'district'\n",
    "df_num_unique_districts = df['District'].nunique()\n",
    "print(f\"Number of unique districts: {df_num_unique_districts}\")\n",
    "\n",
    "# Count occurrences of each unique value in 'district'\n",
    "df_district_counts = df['District'].value_counts()\n",
    "print(\"Counts of each district:\")\n",
    "print(df_district_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "83af64c1-0e8c-4ae4-969b-ab6ee0a4ea31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'df'\n",
    "df = df[df['District'] != 31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af216509-dd60-42fd-9a41-e1db581c232d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique districts: 22\n",
      "Counts of each district:\n",
      "11    36109\n",
      "6     31722\n",
      "8     30737\n",
      "18    30541\n",
      "1     29954\n",
      "4     26414\n",
      "7     26395\n",
      "12    25182\n",
      "25    25139\n",
      "10    23968\n",
      "3     23386\n",
      "19    23336\n",
      "2     21911\n",
      "5     21554\n",
      "9     20932\n",
      "15    19206\n",
      "14    17970\n",
      "16    16359\n",
      "22    15774\n",
      "24    15084\n",
      "17    13866\n",
      "20     9017\n",
      "Name: District, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count unique values in 'district'\n",
    "df_num_unique_districts = df['District'].nunique()\n",
    "print(f\"Number of unique districts: {df_num_unique_districts}\")\n",
    "\n",
    "# Count occurrences of each unique value in 'district'\n",
    "df_district_counts = df['District'].value_counts()\n",
    "print(\"Counts of each district:\")\n",
    "print(df_district_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d3603358-34ef-4c08-bd91-9c0bc69ed67f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_stamp                        object\n",
      "IUCR                              object\n",
      "primary_type                      object\n",
      "latitude                         float64\n",
      "longitude                        float64\n",
      "month                              int64\n",
      "season                            object\n",
      "year                               int64\n",
      "district                           int64\n",
      "beat                               int64\n",
      "geometry                          object\n",
      "community_name                    object\n",
      "community_area_number              int64\n",
      "NIBRS                             object\n",
      "description                       object\n",
      "category                          object\n",
      "crime_against_category            object\n",
      "district_name                     object\n",
      "temperature_2m_max_day           float64\n",
      "temperature_2m_min_day           float64\n",
      "apparent_temperature_mean_day    float64\n",
      "daylight_duration_day            float64\n",
      "sunshine_duration_day            float64\n",
      "precipitation_sum_day            float64\n",
      "rain_sum_day                     float64\n",
      "snowfall_sum_day                 float64\n",
      "precipitation_hours_day          float64\n",
      "apparent_temperature_hour        float64\n",
      "precipitation_hour               float64\n",
      "rain_hour                        float64\n",
      "snowfall_hour                    float64\n",
      "cloud_cover_hour                 float64\n",
      "wind_speed_100m_hour             float64\n",
      "hour                               int64\n",
      "day_of_week                        int64\n",
      "tot_pop                          float64\n",
      "unemp                            float64\n",
      "inc_lt_25k                       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#clean column names\n",
    "df.columns\n",
    "df.rename(columns={\n",
    "    'Date': 'time_stamp',    \n",
    "    'Hour': 'hour',\n",
    "    'DayOfWeek': 'day_of_week',\n",
    "    'Month': 'month',\n",
    "    'Season': 'season',\n",
    "    'Year': 'year',\n",
    "    \n",
    "    'Primary Type': 'primary_type',\n",
    "    'Description': 'description',\n",
    "    'Category':'category',\n",
    "    'Crime Against Category':'crime_against_category', \n",
    "    \n",
    "    'Latitude': 'latitude',\n",
    "    'Longitude': 'longitude',\n",
    "    'District':'district', \n",
    "    'Beat': 'beat',\n",
    "    'Community Name': 'community_name',\n",
    "    'Community Area Number': 'community_area_number',\n",
    "    \n",
    "    'TOT_POP': 'tot_pop',\n",
    "    'UNEMP': 'unemp',\n",
    "    'INC_LT_25K': 'inc_lt_25k'\n",
    "}, inplace=True)\n",
    "\n",
    "#clean datatypes\n",
    "print(df.dtypes)\n",
    "\n",
    "df['time_stamp'] = pd.to_datetime(df['time_stamp'])\n",
    "categorical_columns = ['season', 'primary_type', 'category', 'crime_against_category', 'community_name']\n",
    "df[categorical_columns] = df[categorical_columns].astype('category')\n",
    "\n",
    "#new variable: date\n",
    "df['date'] = df['time_stamp'].dt.date\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "#new variable: week_in_year\n",
    "df['week_in_year'] = df['date'].dt.strftime('%Y-%U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9877a382-edaa-4a74-819b-74526638cf39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   district week_in_year  total_crimes  temperature_2m_max_day  \\\n",
      "0         1      2018-00           219              -11.143499   \n",
      "1         1      2018-01           279                4.006500   \n",
      "2         1      2018-02           259               -2.793500   \n",
      "3         1      2018-03           292                6.856500   \n",
      "4         1      2018-04           293               -0.343500   \n",
      "\n",
      "   temperature_2m_min_day  apparent_temperature_mean_day  \\\n",
      "0              -17.693500                     -21.635940   \n",
      "1               -4.293500                      -5.290957   \n",
      "2              -12.643499                     -14.063271   \n",
      "3                0.406500                      -0.672545   \n",
      "4               -7.293500                     -10.021296   \n",
      "\n",
      "   daylight_duration_day  sunshine_duration_day  precipitation_sum_day  \\\n",
      "0              33273.793             28849.9880                    0.0   \n",
      "1              33691.383              5621.2524                    0.5   \n",
      "2              34341.723             29628.3070                    0.0   \n",
      "3              35118.900              8625.8150                    0.5   \n",
      "4              36024.040             19594.2230                    0.0   \n",
      "\n",
      "   rain_sum_day  ...  precipitation_hours_day  apparent_temperature_hour  \\\n",
      "0           0.0  ...                      0.0                 -21.732040   \n",
      "1           0.1  ...                      4.0                  -6.278983   \n",
      "2           0.0  ...                      0.0                 -13.492929   \n",
      "3           0.1  ...                      1.0                  -0.382499   \n",
      "4           0.0  ...                      0.0                 -10.102794   \n",
      "\n",
      "   precipitation_hour  rain_hour  snowfall_hour  cloud_cover_hour  \\\n",
      "0                 0.0        0.0            0.0               3.6   \n",
      "1                 0.0        0.0            0.0             100.0   \n",
      "2                 0.0        0.0            0.0               7.8   \n",
      "3                 0.0        0.0            0.0              92.4   \n",
      "4                 0.0        0.0            0.0              38.4   \n",
      "\n",
      "   wind_speed_100m_hour  tot_pop   unemp  inc_lt_25k  \n",
      "0             28.008370  35010.0  1172.0      3120.0  \n",
      "1             35.712650  35010.0  1172.0      3120.0  \n",
      "2             31.877792  35010.0  1172.0      3120.0  \n",
      "3             28.257132  35010.0  1172.0      3120.0  \n",
      "4             31.259941  35010.0  1172.0      3120.0  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "      district week_in_year  total_crimes  temperature_2m_max_day  \\\n",
      "2327        25      2019-48           256                4.756500   \n",
      "2328        25      2019-49           227                6.456500   \n",
      "2329        25      2019-50           195                0.506500   \n",
      "2330        25      2019-51           237               10.356501   \n",
      "2331        25      2019-52            82                9.456500   \n",
      "\n",
      "      temperature_2m_min_day  apparent_temperature_mean_day  \\\n",
      "2327                 -1.2935                      -3.547280   \n",
      "2328                 -3.1935                      -4.868554   \n",
      "2329                 -6.4935                      -8.737647   \n",
      "2330                  0.2565                      -0.116335   \n",
      "2331                 -1.4935                      -5.180399   \n",
      "\n",
      "      daylight_duration_day  sunshine_duration_day  precipitation_sum_day  \\\n",
      "2327              33493.490              15701.918               0.000000   \n",
      "2328              33100.387              20670.336               0.000000   \n",
      "2329              32891.150              28320.006               0.000000   \n",
      "2330              32887.184              25127.332               0.000000   \n",
      "2331              33018.312              10834.614               6.999999   \n",
      "\n",
      "      rain_sum_day  ...  precipitation_hours_day  apparent_temperature_hour  \\\n",
      "2327           0.0  ...                      0.0                  -3.492873   \n",
      "2328           0.0  ...                      0.0                  -3.835857   \n",
      "2329           0.0  ...                      0.0                  -7.503908   \n",
      "2330           0.0  ...                      0.0                   0.190635   \n",
      "2331           6.0  ...                     11.0                  -4.748684   \n",
      "\n",
      "      precipitation_hour  rain_hour  snowfall_hour  cloud_cover_hour  \\\n",
      "2327                 0.0        0.0            0.0         46.350000   \n",
      "2328                 0.0        0.0            0.0         66.900000   \n",
      "2329                 0.0        0.0            0.0         29.400002   \n",
      "2330                 0.0        0.0            0.0         27.900002   \n",
      "2331                 0.0        0.0            0.0        100.000000   \n",
      "\n",
      "      wind_speed_100m_hour  tot_pop   unemp  inc_lt_25k  \n",
      "2327             30.157369  79910.0  3763.0      5438.0  \n",
      "2328             28.137632  79910.0  3763.0      5438.0  \n",
      "2329             22.828087  79910.0  3763.0      5438.0  \n",
      "2330             28.876396  79910.0  3763.0      5438.0  \n",
      "2331             37.146652  79910.0  3763.0      5438.0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "#preparation heatmap: group data by week and district\n",
    "num_features = [\n",
    "    'temperature_2m_max_day', 'temperature_2m_min_day', 'apparent_temperature_mean_day',\n",
    "    'daylight_duration_day', 'sunshine_duration_day', 'precipitation_sum_day',\n",
    "    'rain_sum_day', 'snowfall_sum_day', 'precipitation_hours_day', \n",
    "    'apparent_temperature_hour', 'precipitation_hour', 'rain_hour', \n",
    "    'snowfall_hour', 'cloud_cover_hour', 'wind_speed_100m_hour', \n",
    "    'tot_pop', 'unemp', 'inc_lt_25k'\n",
    "]\n",
    "\n",
    "#aggregation of the data by district and week, and calculation of the median\n",
    "df['crimes_per_week'] = 1  #add a helper column to count the crimes\n",
    "\n",
    "#aggregation of the data by district and week, and calculation of the median\n",
    "data = df.groupby(['district', 'week_in_year']).agg(\n",
    "    total_crimes=('crimes_per_week', 'sum'),  #count the number of crimes\n",
    "    **{var: (var, 'median') for var in num_features}  #calculate the median of numerical features\n",
    ").reset_index()\n",
    "\n",
    "#output the aggregated df\n",
    "print(data.head())  # Print the first few rows to check sorting\n",
    "print(data.tail())  # Print the last few rows to ensure it's sorted as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76c75fcb-7119-49d0-a126-c9fd4fffde54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_heatmap_array(data, map_size=(256, 256), radius=5):\n",
    "    \"\"\"\n",
    "    creates a heatmap as a numpy array based on latitude and longitude\n",
    "    \n",
    "    parameters:\n",
    "    - data: df with the columns 'latitude', 'longitude', 'crimes_per_week'\n",
    "    - map_size: size of the heatmap (height, width)\n",
    "    - radius: radius of the heatmap points\n",
    "    \n",
    "    returns:\n",
    "    - heatmap_array: numpy array of the specified map_size\n",
    "    \"\"\"\n",
    "    #initialize array\n",
    "    heatmap_array = np.zeros(map_size)\n",
    "\n",
    "    #normalize latitude and longitude to map size\n",
    "    lat_min, lat_max = data['latitude'].min(), data['latitude'].max()\n",
    "    lon_min, lon_max = data['longitude'].min(), data['longitude'].max()\n",
    "\n",
    "    #check if lat_max equals lat_min or lon_max equals lon_min\n",
    "    if lat_max == lat_min:\n",
    "        lat_max += 0.001  #adjust slightly to avoid zero division\n",
    "    if lon_max == lon_min:\n",
    "        lon_max += 0.001  #adjust slightly to avoid zero division\n",
    "\n",
    "    #scaling function\n",
    "    def scale_lat_lon(lat, lon):\n",
    "        x = int((lon - lon_min) / (lon_max - lon_min) * (map_size[1] - 1))\n",
    "        y = int((lat - lat_min) / (lat_max - lat_min) * (map_size[0] - 1))\n",
    "        return x, y\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        x, y = scale_lat_lon(row['latitude'], row['longitude'])\n",
    "        heatmap_array[y, x] += row['crimes_per_week']\n",
    "\n",
    "    return heatmap_array\n",
    "\n",
    "# create a heatmap for a week and a district, and resize it to 256x256\n",
    "def create_and_resize_heatmap(week_data, map_size=(256, 256), radius=5):\n",
    "    heatmap_array = create_heatmap_array(week_data, map_size=map_size, radius=radius)\n",
    "    heatmap_resized = cv2.resize(heatmap_array, map_size)\n",
    "    return heatmap_resized\n",
    "\n",
    "# list of variables for which the median was calculated\n",
    "num_features = [\n",
    "    'temperature_2m_max_day', 'temperature_2m_min_day', 'apparent_temperature_mean_day',\n",
    "    'daylight_duration_day', 'sunshine_duration_day', 'precipitation_sum_day',\n",
    "    'rain_sum_day', 'snowfall_sum_day', 'precipitation_hours_day', \n",
    "    'apparent_temperature_hour', 'precipitation_hour', 'rain_hour', \n",
    "    'snowfall_hour', 'cloud_cover_hour', 'wind_speed_100m_hour', \n",
    "    'tot_pop', 'unemp', 'inc_lt_25k'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde86642-8db4-4920-b45d-62bcaade2189",
   "metadata": {},
   "source": [
    "X_images_sequences: This will store the sequences of heatmaps (or images) for all districts, for the last window_size weeks.\n",
    "X_socio_sequences: This will store the sequences of socio-economic data for all districts, for the last window_size weeks.\n",
    "y_labels_sequences: This will store the crime counts for all districts, but only for the week following the window_size weeks.\n",
    "For instance, if you have 23 districts and a window size of 10 weeks, then:\n",
    "\n",
    "Each element in X_images_sequences will have shape (23, 10, 256, 256, 1).\n",
    "Each element in X_socio_sequences will have shape (23, 10, num_features).\n",
    "Each element in y_labels_sequences will be a 1D array of size 23, where each value corresponds to the crime count for each district in the week following the 10-week window.\n",
    "Example:\n",
    "Let's say you're processing data for district 1, and your current window_size is 10:\n",
    "\n",
    "X_images_sequences[i]: Contains the heatmap data for district 1 across the last 10 weeks.\n",
    "X_socio_sequences[i]: Contains the socio-economic data for district 1 across the last 10 weeks.\n",
    "y_labels_sequences[i]: Contains the crime count for district 1 in the week after these 10 weeks.\n",
    "When combining all districts:\n",
    "\n",
    "X_images_sequences[j]: Contains sequences for all 23 districts.\n",
    "y_labels_sequences[j]: Contains the crime counts for all 23 districts for the week following the sequence.\n",
    "So, after you complete the sequence generation for all districts and all time windows, y_labels_sequences will hold the crime counts for every district for the relevant weeks, allowing you to predict and compare the actual crime counts per district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11a2b871-59f6-48ec-81d6-7eb858167577",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 22 24 25]\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# Double checking district number stays the same\n",
    "districts = data['district'].unique()\n",
    "num_districts = len(districts)\n",
    "print(districts)\n",
    "print(num_districts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c524a708-b0be-4f09-a785-91eac5e1af74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "districts = data['district'].unique()\n",
    "# Initialize your lists\n",
    "X_images_sequences = []\n",
    "X_socio_sequences = []\n",
    "y_labels_sequences = []\n",
    "district_mapping = []  # This will store the district for each sequence\n",
    "\n",
    "window_size = 10  # number of weeks in the sliding window\n",
    "max_weeks = 159   # maximum number of weeks to be processed\n",
    "\n",
    "# Iterate over all districts\n",
    "for neighborhood in districts:\n",
    "    neighborhood_data = data[data['district'] == neighborhood]\n",
    "    neighborhood_data = neighborhood_data.sort_values('week_in_year')\n",
    "    \n",
    "    # Limit to the first XX weeks\n",
    "    neighborhood_data = neighborhood_data.head(max_weeks)\n",
    "    \n",
    "    for i in range(len(neighborhood_data) - window_size):\n",
    "        window_data = neighborhood_data.iloc[i:i + window_size]\n",
    "        next_week_data = neighborhood_data.iloc[i + window_size]\n",
    "        \n",
    "        # Create heatmap sequence\n",
    "        heatmaps_sequence = np.array([\n",
    "            create_and_resize_heatmap(df[(df['district'] == neighborhood) & (df['week_in_year'] == week)])\n",
    "            for week in window_data['week_in_year']\n",
    "        ])\n",
    "        \n",
    "        # Create socio-economic feature sequence\n",
    "        socio_sequence = window_data[num_features].values\n",
    "        \n",
    "        # Append sequences\n",
    "        X_images_sequences.append(heatmaps_sequence)\n",
    "        X_socio_sequences.append(socio_sequence)\n",
    "        \n",
    "        # Store the crime counts for all districts in the target week\n",
    "        all_districts_crime_counts = data[data['week_in_year'] == next_week_data['week_in_year']]['total_crimes'].values\n",
    "        y_labels_sequences.append(all_districts_crime_counts)\n",
    "        \n",
    "        # Track the district (optional if you need this mapping)\n",
    "        district_mapping.append(neighborhood)  # Store the current district\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ae18662-7dec-4ad7-aece-3e6d16e606df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to /home/scc/miranda.barros-everett/model/2018_2019_with_district_data_final_without_d31.pkl.\n"
     ]
    }
   ],
   "source": [
    "# Save the new pickle file\n",
    "save_path = r\"/home/scc/miranda.barros-everett/model/2018_2019_with_district_data_final_without_d31.pkl\"\n",
    "\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump((X_images_sequences, X_socio_sequences, y_labels_sequences), f)\n",
    "\n",
    "print(f\"Data has been successfully saved to {save_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06906b51-bfd1-4e49-987c-627fbd51dd25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes before normalization:\n",
      "X_images_sequences shape: (2112, 10, 256, 256, 1)\n",
      "X_socio_sequences shape: (2112, 10, 18)\n",
      "y_labels_sequences shape: (2112, 22)\n",
      "Shapes after normalization:\n",
      "X_images_sequences shape: (2112, 10, 256, 256, 1)\n",
      "X_socio_sequences shape: (2112, 10, 18)\n",
      "y_labels_sequences shape: (2112, 22)\n",
      "Shapes after train/test split:\n",
      "X_images_train shape: (1900, 10, 256, 256, 1)\n",
      "X_images_test shape: (212, 10, 256, 256, 1)\n",
      "X_socio_train shape: (1900, 10, 18)\n",
      "X_socio_test shape: (212, 10, 18)\n",
      "y_train shape: (1900, 22)\n",
      "y_test shape: (212, 22)\n"
     ]
    }
   ],
   "source": [
    "# Convert lists to numpy arrays\n",
    "X_images_sequences = np.array(X_images_sequences)\n",
    "X_socio_sequences = np.array(X_socio_sequences)\n",
    "\n",
    "# Check the length of sequences in y_labels_sequences\n",
    "#sequence_lengths = [len(seq) for seq in y_labels_sequences]\n",
    "#print(\"Sequence lengths in y_labels_sequences:\", sequence_lengths)\n",
    "\n",
    "# Determine the maximum length\n",
    "#max_length = max(sequence_lengths)\n",
    "#print(\"Maximum length of sequences:\", max_length)\n",
    "\n",
    "# Pad y_labels_sequences to ensure consistent shape\n",
    "#padded_y_labels_sequences = np.array([\n",
    "#    np.pad(seq, (0, max_length - len(seq)), mode='constant', constant_values=0) if len(seq) < max_length\n",
    "#    else seq\n",
    "#    for seq in y_labels_sequences\n",
    "#])\n",
    "\n",
    "# Print shapes of the data arrays\n",
    "print(\"Shapes before normalization:\")\n",
    "print(\"X_images_sequences shape:\", X_images_sequences.shape)\n",
    "print(\"X_socio_sequences shape:\", X_socio_sequences.shape)\n",
    "print(\"y_labels_sequences shape:\", padded_y_labels_sequences.shape)\n",
    "\n",
    "# Normalize the socio-economic data\n",
    "scaler_socio = StandardScaler()\n",
    "X_socio_sequences = scaler_socio.fit_transform(X_socio_sequences.reshape(-1, X_socio_sequences.shape[-1])).reshape(X_socio_sequences.shape)\n",
    "\n",
    "# Normalize the crime counts for each district separately\n",
    "scaler_y_labels = StandardScaler()\n",
    "# Reshape padded_y_labels_sequences to 2D (number of samples x number of districts)\n",
    "y_labels_flat = padded_y_labels_sequences.reshape(-1, padded_y_labels_sequences.shape[-1])\n",
    "y_labels_normalized = scaler_y_labels.fit_transform(y_labels_flat)\n",
    "\n",
    "# Reshape back to the original 3D shape (number of samples x window_size x number of districts)\n",
    "y_labels_sequences = y_labels_normalized.reshape(padded_y_labels_sequences.shape)\n",
    "\n",
    "# Print shapes after normalization\n",
    "print(\"Shapes after normalization:\")\n",
    "print(\"X_images_sequences shape:\", X_images_sequences.shape)\n",
    "print(\"X_socio_sequences shape:\", X_socio_sequences.shape)\n",
    "print(\"y_labels_sequences shape:\", y_labels_sequences.shape)\n",
    "\n",
    "# Prepare data for training\n",
    "X_images_sequences = X_images_sequences.reshape((X_images_sequences.shape[0], window_size, 256, 256, 1))\n",
    "\n",
    "# Split the data\n",
    "X_images_train, X_images_test, X_socio_train, X_socio_test, y_train, y_test = train_test_split(\n",
    "    X_images_sequences, X_socio_sequences, y_labels_sequences, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Print shapes after splitting\n",
    "print(\"Shapes after train/test split:\")\n",
    "print(\"X_images_train shape:\", X_images_train.shape)\n",
    "print(\"X_images_test shape:\", X_images_test.shape)\n",
    "print(\"X_socio_train shape:\", X_socio_train.shape)\n",
    "print(\"X_socio_test shape:\", X_socio_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63a30386-d75c-4a72-bee1-0c97d8186a01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 10, 256, 256, 1)]    0         []                            \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 10, 256, 256, 32)     320       ['input_1[0][0]']             \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDi  (None, 10, 256, 256, 32)     128       ['time_distributed[0][0]']    \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDi  (None, 10, 128, 128, 32)     0         ['time_distributed_1[0][0]']  \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " time_distributed_3 (TimeDi  (None, 10, 128, 128, 64)     18496     ['time_distributed_2[0][0]']  \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " time_distributed_4 (TimeDi  (None, 10, 128, 128, 64)     256       ['time_distributed_3[0][0]']  \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " time_distributed_5 (TimeDi  (None, 10, 64, 64, 64)       0         ['time_distributed_4[0][0]']  \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " time_distributed_6 (TimeDi  (None, 10, 64, 64, 128)      73856     ['time_distributed_5[0][0]']  \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " time_distributed_7 (TimeDi  (None, 10, 64, 64, 128)      512       ['time_distributed_6[0][0]']  \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " time_distributed_8 (TimeDi  (None, 10, 32, 32, 128)      0         ['time_distributed_7[0][0]']  \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " time_distributed_9 (TimeDi  (None, 10, 131072)           0         ['time_distributed_8[0][0]']  \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 10, 18)]             0         []                            \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 (None, 10, 50)               2622460   ['time_distributed_9[0][0]']  \n",
      "                                                          0                                       \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               (None, 10, 50)               13800     ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               (None, 50)                   20200     ['lstm[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               (None, 50)                   20200     ['lstm_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 50)                   0         ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 50)                   0         ['lstm_3[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 100)                  0         ['dropout[0][0]',             \n",
      "                                                                     'dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 50)                   5050      ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 50)                   0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 22)                   1122      ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26378540 (100.63 MB)\n",
      "Trainable params: 26378092 (100.62 MB)\n",
      "Non-trainable params: 448 (1.75 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "60/60 [==============================] - 157s 2s/step - loss: 7.1957 - val_loss: 5.3741 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "60/60 [==============================] - 128s 2s/step - loss: 4.8532 - val_loss: 4.2867 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "60/60 [==============================] - 122s 2s/step - loss: 4.0968 - val_loss: 3.7465 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "60/60 [==============================] - 119s 2s/step - loss: 3.6354 - val_loss: 3.3413 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "60/60 [==============================] - 123s 2s/step - loss: 3.2655 - val_loss: 2.9892 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "60/60 [==============================] - 132s 2s/step - loss: 2.9524 - val_loss: 2.6391 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "60/60 [==============================] - 132s 2s/step - loss: 2.7010 - val_loss: 2.4235 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "60/60 [==============================] - 132s 2s/step - loss: 2.4731 - val_loss: 2.1976 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "60/60 [==============================] - 131s 2s/step - loss: 2.2882 - val_loss: 1.9900 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "60/60 [==============================] - 131s 2s/step - loss: 2.1175 - val_loss: 1.7889 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "60/60 [==============================] - 132s 2s/step - loss: 1.9658 - val_loss: 1.6511 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "60/60 [==============================] - 131s 2s/step - loss: 1.8316 - val_loss: 1.5230 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "60/60 [==============================] - 129s 2s/step - loss: 1.7162 - val_loss: 1.4241 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "60/60 [==============================] - 128s 2s/step - loss: 1.6240 - val_loss: 1.3398 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "60/60 [==============================] - 128s 2s/step - loss: 1.5388 - val_loss: 1.2657 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "60/60 [==============================] - 127s 2s/step - loss: 1.4621 - val_loss: 1.1990 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "60/60 [==============================] - 126s 2s/step - loss: 1.3966 - val_loss: 1.1413 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "60/60 [==============================] - 125s 2s/step - loss: 1.3390 - val_loss: 1.0895 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "60/60 [==============================] - 125s 2s/step - loss: 1.2877 - val_loss: 1.0424 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "60/60 [==============================] - 127s 2s/step - loss: 1.2439 - val_loss: 1.0026 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "60/60 [==============================] - 128s 2s/step - loss: 1.2030 - val_loss: 0.9677 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "60/60 [==============================] - 128s 2s/step - loss: 1.1687 - val_loss: 0.9345 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "60/60 [==============================] - 126s 2s/step - loss: 1.1370 - val_loss: 0.9054 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "60/60 [==============================] - 125s 2s/step - loss: 1.1089 - val_loss: 0.8786 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "60/60 [==============================] - 126s 2s/step - loss: 1.0813 - val_loss: 0.8558 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "60/60 [==============================] - 130s 2s/step - loss: 1.0583 - val_loss: 0.8358 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "60/60 [==============================] - 133s 2s/step - loss: 1.0400 - val_loss: 0.8167 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "60/60 [==============================] - 131s 2s/step - loss: 1.0218 - val_loss: 0.8016 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "60/60 [==============================] - 131s 2s/step - loss: 1.0054 - val_loss: 0.7872 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "60/60 [==============================] - 132s 2s/step - loss: 0.9906 - val_loss: 0.7755 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "60/60 [==============================] - 131s 2s/step - loss: 0.9768 - val_loss: 0.7610 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "60/60 [==============================] - 130s 2s/step - loss: 0.9655 - val_loss: 0.7507 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "60/60 [==============================] - 130s 2s/step - loss: 0.9549 - val_loss: 0.7438 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "60/60 [==============================] - 129s 2s/step - loss: 0.9452 - val_loss: 0.7339 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "60/60 [==============================] - 129s 2s/step - loss: 0.9364 - val_loss: 0.7298 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "60/60 [==============================] - 127s 2s/step - loss: 0.9293 - val_loss: 0.7236 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "60/60 [==============================] - 126s 2s/step - loss: 0.9249 - val_loss: 0.7178 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "60/60 [==============================] - 125s 2s/step - loss: 0.9170 - val_loss: 0.7121 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "60/60 [==============================] - 125s 2s/step - loss: 0.9123 - val_loss: 0.7075 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "60/60 [==============================] - 126s 2s/step - loss: 0.9082 - val_loss: 0.7046 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "60/60 [==============================] - 127s 2s/step - loss: 0.9028 - val_loss: 0.6996 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "60/60 [==============================] - 126s 2s/step - loss: 0.8997 - val_loss: 0.6965 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "60/60 [==============================] - 126s 2s/step - loss: 0.8960 - val_loss: 0.6961 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "60/60 [==============================] - 128s 2s/step - loss: 0.8906 - val_loss: 0.6941 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "60/60 [==============================] - 132s 2s/step - loss: 0.8908 - val_loss: 0.6918 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "60/60 [==============================] - 135s 2s/step - loss: 0.8863 - val_loss: 0.6865 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "60/60 [==============================] - 133s 2s/step - loss: 0.8831 - val_loss: 0.6870 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "60/60 [==============================] - 131s 2s/step - loss: 0.8804 - val_loss: 0.6822 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "60/60 [==============================] - 128s 2s/step - loss: 0.8790 - val_loss: 0.6831 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "60/60 [==============================] - 129s 2s/step - loss: 0.8772 - val_loss: 0.6811 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "60/60 [==============================] - 131s 2s/step - loss: 0.8749 - val_loss: 0.6801 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "60/60 [==============================] - 130s 2s/step - loss: 0.8729 - val_loss: 0.6779 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "60/60 [==============================] - 128s 2s/step - loss: 0.8706 - val_loss: 0.6784 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "60/60 [==============================] - 126s 2s/step - loss: 0.8694 - val_loss: 0.6794 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "60/60 [==============================] - 124s 2s/step - loss: 0.8659 - val_loss: 0.6786 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "60/60 [==============================] - 124s 2s/step - loss: 0.8677 - val_loss: 0.6814 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "60/60 [==============================] - 127s 2s/step - loss: 0.8679 - val_loss: 0.6795 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, LSTM, Dropout, Dense, concatenate, BatchNormalization, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model architecture\n",
    "input_images = Input(shape=(window_size, 256, 256, 1))\n",
    "\n",
    "# CNN layers with batch normalization \n",
    "x = TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)))(input_images)\n",
    "x = TimeDistributed(BatchNormalization())(x)\n",
    "x = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(x)\n",
    "x = TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)))(x)\n",
    "x = TimeDistributed(BatchNormalization())(x)\n",
    "x = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(x)\n",
    "x = TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)))(x)\n",
    "x = TimeDistributed(BatchNormalization())(x)\n",
    "x = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(x)\n",
    "x = TimeDistributed(Flatten())(x)\n",
    "\n",
    "# LSTM layers on image features\n",
    "x = LSTM(50, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
    "x = LSTM(50, activation='tanh', kernel_regularizer=l2(0.01))(x)\n",
    "x = Dropout(0.5)(x) \n",
    "\n",
    "input_socio = Input(shape=(window_size, X_socio_train.shape[2]))\n",
    "\n",
    "# LSTM layer on socio-economic data \n",
    "y = LSTM(50, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.01))(input_socio)\n",
    "y = LSTM(50, activation='tanh', kernel_regularizer=l2(0.01))(y)\n",
    "y = Dropout(0.5)(y)  \n",
    "\n",
    "combined = concatenate([x, y])\n",
    "\n",
    "# Fully connected layers with dropout and L2 regularization\n",
    "z = Dense(50, activation='relu', kernel_regularizer=l2(0.01))(combined)\n",
    "z = Dropout(0.5)(z)  \n",
    "\n",
    "# Output layer modified to predict per district\n",
    "#output = Dense(num_districts)(z)\n",
    "#output = Dense(num_districts, activation='relu')(z)\n",
    "# alternative two:\n",
    "output = Dense(num_districts, activation='softplus')(z)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[input_images, input_socio], outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='mse')\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n",
    "\n",
    "# Callbacks for dynamic learning rate adjustment and early stopping\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    [X_images_train, X_socio_train], y_train,\n",
    "    validation_data=([X_images_test, X_socio_test], y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Save the model to a specific folder\n",
    "model.save(r\"/home/scc/miranda.barros-everett/model/saved_models/all_crimes_model_2018_2019_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1158e41d-6ce8-45dd-b814-543e37a9dbb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_test: (212, 22)\n",
      "Shape of y_pred_sequences: (212, 22)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "print(\"Shape of y_pred_sequences:\", y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "063eb5b3-4ac0-41a7-bbe3-7e593e321050",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 6s 545ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([X_images_test, X_socio_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af2dd523-7495-41b5-8c2a-cc39f5f5f21e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.649365800199699\n",
      "Mean Squared Error on original scale: 0.649365800199699\n",
      "R-squared Score: 0.19914357217676348\n",
      "Mean of y_test: -5.767477552744513e-05\n",
      "Mean of y_pred: 0.25377318\n",
      "Variance of y_test: 0.8133213429903979\n",
      "Variance of y_pred: 0.08802336\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Inverse-transform predictions and true values\n",
    "#y_test_original = scaler_y_labels.inverse_transform(y_test)\n",
    "#y_pred_original = scaler_y_labels.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate MSE on the original scale\n",
    "mse_original = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on original scale:\", mse_original)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared Score:\", r2)\n",
    "\n",
    "print(\"Mean of y_test:\", np.mean(y_test))\n",
    "print(\"Mean of y_pred:\", np.mean(y_pred))\n",
    "\n",
    "print(\"Variance of y_test:\", np.var(y_test))\n",
    "print(\"Variance of y_pred:\", np.var(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3ef8263-5872-4131-b507-70f460858c09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Values (First few samples):\n",
      "[[-0.04041443  1.17526459  0.35156033  0.61473745  0.78182218  1.46636315\n",
      "   0.7436697   0.22155504  0.10782744  1.06977434  1.41464314  0.54328675\n",
      "   0.56386732  0.8770466   1.13073047 -0.4658522   0.30439755 -0.17840121\n",
      "   0.2602474   1.50456844  1.64911085  1.03045172]]\n",
      "Predicted Values (First few samples):\n",
      "[[0.64266753 0.7280608  0.6890951  0.6994651  0.7097076  0.749153\n",
      "  0.73813343 0.6695309  0.6905838  0.6551253  0.6509831  0.66295606\n",
      "  0.6328733  0.6894588  0.54757524 0.57337064 0.61229163 0.658701\n",
      "  0.63499624 0.683954   0.7019525  0.6062955 ]]\n"
     ]
    }
   ],
   "source": [
    "num_samples_to_show = 1  # Number of samples to display\n",
    "print(\"True Values (First few samples):\")\n",
    "print(y_test[:num_samples_to_show])\n",
    "\n",
    "print(\"Predicted Values (First few samples):\")\n",
    "print(y_pred[:num_samples_to_show])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98a644f6-b33b-41f3-95a4-6b825c7f42c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnormalized True Values (First few samples):\n",
      "[[282. 252. 236. 275. 233. 371. 286. 301. 203. 266. 422. 264. 190. 210.\n",
      "  180. 120. 305. 216.  90. 186. 187. 273.]]\n",
      "Unnormalized Predicted Values (First few samples):\n",
      "[[317.0363  235.43974 248.55138 278.17004 230.57584 338.20938 285.75394\n",
      "  319.32404 221.91585 251.66672 380.34912 268.8092  192.10596 204.29861\n",
      "  166.54796 143.1213  319.07654 249.04123  96.62987 166.45027 162.33907\n",
      "  258.6147 ]]\n"
     ]
    }
   ],
   "source": [
    "# Unnormalize predictions\n",
    "y_pred_flat = y_pred.reshape(-1, y_pred.shape[-1])\n",
    "y_pred_original = scaler_y_labels.inverse_transform(y_pred_flat)\n",
    "y_pred_sequences = y_pred_original.reshape(y_pred.shape)\n",
    "\n",
    "# Unnormalize true values\n",
    "y_test_flat = y_test.reshape(-1, y_test.shape[-1])\n",
    "y_test_original = scaler_y_labels.inverse_transform(y_test_flat)\n",
    "y_test_sequences = y_test_original.reshape(y_test.shape)\n",
    "\n",
    "# Print some values for comparison\n",
    "num_samples_to_show = 1 # Number of samples to display\n",
    "\n",
    "print(\"Unnormalized True Values (First few samples):\")\n",
    "print(y_test_sequences[:num_samples_to_show])\n",
    "\n",
    "print(\"Unnormalized Predicted Values (First few samples):\")\n",
    "print(y_pred_sequences[:num_samples_to_show])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827fae2-ae8b-42db-9efb-f3f3a5d9b0bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95b89e0-f279-4fe2-b7dc-467a815be432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45145944-6c5c-4253-98bb-623c9df07ca9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2ebdd-3001-4e3b-9e59-4659db63843e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaea0a8-0a8a-4e7d-b374-7e6f3663d736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f009c70-92ee-467d-a479-30aac085a172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf82e7-1e89-4905-ac8e-2ab67910ad96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
